15 サポートベクターマシンと文書の機械学習
==========================================

2013年8月28日（水）

[資料（英語）](http://nlp.stanford.edu/IR-book/pdf/15svm.pdf)


序
--

__サポートベクターマシン (SVM)__

- ラージマージン分類機の一種
- ベクトル空間に基づいた機械学習法
- 訓練データ（いくつかの点を外れ値もしくはノイズとして除く）のあらゆる点から最も離れている2つのクラスの間の決定境界を見つける


15.1 サポートベクターマシン -- 線形分離可能な場合
--------------------------------------------------

__SVM__ ...あらゆるデータ点からの距離が最大となる決定面を求める基準を定義

![サポートベクターは、分類器のマージンのすぐそばにある5点である。](./15/fig15.1.png)

- __最大マージン決定超平面__ _(Maximum decision hyperplane)_ ...実線の直線
- __分離面__...点線の直線?
- 分離面の __マージン__ _(margin)_ ...決定面から最近データ点に接するまでの距離
- __サポートベクター__ _(support vector)_ ...分離面の位置を定義するデータ（原点からその点までのベクトル）


### マージン最大化

__マージン最大化__ ...マージンが最大になるように分類面を決定すること?

- 分類に安全マージンを与える
 - 分離面にわずかな誤差があっても分類誤りを引き起こさない


### SVMの定式化

- _b_ ...定数項
- _w_ ...決定超平面法線ベクトル（重みベクトル）
 - 超平面に垂直

決定超平面は _b_ と _w_ で定義される。その方程式は

![(14.3)式 決定超平面の定義](./14/eq14.3.png)

となる（(14.3)式）。超平面が法線ベクトルに垂直なので、超平面上の全ての点 _x_ は

    Tw x = -b

を満たす。（これは _b_ の符号を逆転させている?）

訓練点集合 __D__ ={( _x\_i_ , _y\_i_ )}

- _x\_i_ ...点
- _y\_i_ ...点 _x\_i_ に対応するラベル

線形分類器は次のようになる。

![(15.1)式 線形分類器](./15/eq15.1.png)

-1という値はあるクラスを示し、1という値は別のクラスを示す。


### マージンの定義

- __関数的マージン__
- __幾何学的マージン__

超平面 < _w_ , _b_ >に関する _i_ 番目の例 _x\_i_
の __関数的マージン__ を量

    y_i (Tw x + b)

と定義する。

決定面に関するデータ集合の関数的マージンを、
最小の関数的マージンをもつ任意のデータ点の関数的マージンの2倍とする。

この定義には問題があり、 _w_ と _b_ のスケールを変えるだけでいくらでも大きくできる。

=> _w_ のサイズに制約を課す必要がある。

![図15.3 点rと決定境界ρとの幾何学的マージン](./15/fig15.3.png)

- _r_ ...点から決定境界へのユークリッド距離
- _x'_ ...超平面上で _x_ に最も近い点

点と超平面との最短距離は、平面への垂線であり _w_ と平行である。
この方向の単位ベクトルは _w/|w|_ である。
図15.3の点線は、ベクトル _rw/|w|_ を表す。
このとき

![x' = x - yrw/|w|](./15/eq15.2.png)

が成り立つ。
さらに、 _x'_ は決定境界にあり _wx'_ + _b_ = 0を満たす。
したがって

![wx'+b=0](./15/eq15.3.png)

となる。これを _r_ について解くと

![r](./15/eq15.4.png)

となる。

分類器の __幾何学的マージン__ _(geometric margin)_ は、
2つのクラスのサポートベクターを分離する幅の最大値である。
すなわち、 _r_ の最小値の2倍となる。

この定義によれば、 _w_ と _b_ のスケールを変えても、
_w_ の長さで正規化されているので、幾何学的マージンは変化しない。

    |w| = 1

と単位ベクトルを選べば、幾何学的マージンと関数的マージンを同じにする。
